import torch
from torch import nn, Tensor, LongTensor
from torch.optim import Adam

from transformers import PreTrainedTokenizer

from typing import Literal

import random
# êµ¬í˜„í•˜ì„¸ìš”!


class Word2Vec(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        d_model: int,
        window_size: int,
        method: Literal["cbow", "skipgram"]
    ) -> None:
        super().__init__()
        self.embeddings = nn.Embedding(vocab_size, d_model, padding_idx=0)
        self.weight = nn.Linear(d_model, vocab_size, bias=False)
        self.window_size = window_size
        self.method = method
        # êµ¬í˜„í•˜ì„¸ìš”!
        #pass

    def embeddings_weight(self) -> Tensor:
        return self.embeddings.weight.detach()

    def fit(
        self,
        corpus: list[str],
        tokenizer: PreTrainedTokenizer,
        lr: float,
        num_epochs: int
    ) -> None:
        criterion = nn.CrossEntropyLoss()
        optimizer = Adam(self.parameters(), lr=lr)
        # êµ¬í˜„í•˜ì„¸ìš”!
        # ğŸ”¹ ë¹ˆ ë¬¸ì¥ ë°©ì§€: tokenized_corpusë¥¼ ìƒì„±í•  ë•Œ í•„í„°ë§ ì¶”ê°€
        tokenized_corpus = []
        for sent in corpus:
            tokenized_sentence = tokenizer.encode(sent, add_special_tokens=False)
            if not tokenized_sentence:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
                continue
            tokenized_corpus.append(tokenized_sentence)

        for epoch in range(num_epochs):
            total_loss: float = 0.0  # ğŸ”¹ ë¶ˆí•„ìš”í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì  ë°©ì§€
            for sentence in tokenized_corpus:
                if len(sentence) < 2:  # ğŸ”¹ ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ì€ í•™ìŠµí•  í•„ìš” ì—†ìŒ
                    continue
                
                loss: Tensor
                if self.method == "cbow":
                    loss = self._train_cbow(sentence, criterion, optimizer)
                else:
                    loss = self._train_skipgram(sentence, criterion, optimizer)

                total_loss += loss.item()  

    def _train_cbow(
        self, sentence: list[int], criterion, optimizer
        # êµ¬í˜„í•˜ì„¸ìš”!
    ) -> Tensor:
        # êµ¬í˜„í•˜ì„¸ìš”!
        context_window = self.window_size // 2
        total_loss: Tensor = torch.tensor(0.0)

        for i in range(context_window, len(sentence) - context_window):
            context = sentence[i - context_window:i] + sentence[i + 1:i + context_window + 1]
            target = sentence[i]

            if target == 0 or len(context) == 0:
                continue
            
            context_embeds = self.embeddings(LongTensor(context)).mean(dim=0)  # í‰ê·  ì„ë² ë”©
            logits = self.weight(context_embeds)
            loss = criterion(logits.unsqueeze(0), LongTensor([target]))

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.detach()

        return total_loss

    def _train_skipgram(
        self, sentence: list[int], criterion, optimizer
        # êµ¬í˜„í•˜ì„¸ìš”!
    ) -> Tensor:
        # êµ¬í˜„í•˜ì„¸ìš”!
        context_window = self.window_size // 2
        total_loss: Tensor = torch.tensor(0.0)

        for i in range(context_window, len(sentence) - context_window):
            target = sentence[i]
            context = sentence[i - context_window:i] + sentence[i + 1:i + context_window + 1]
            
            if target == 0 or len(context) == 0:
                continue


            target_embed = self.embeddings(LongTensor([target]))
            logits = self.weight(target_embed).squeeze(0)

            #context_tensor = torch.tensor(context, dtype=torch.long)
            random_target = random.choice(context)
            target_tensor = torch.tensor([random_target], dtype=torch.long)
            loss = criterion(logits.unsqueeze(0), target_tensor)

            #loss = criterion(logits, LongTensor(context))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss = total_loss + loss.detach()

        return total_loss

    # êµ¬í˜„í•˜ì„¸ìš”!
    #pass